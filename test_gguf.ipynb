{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.39.tar.gz (10.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jinja2>=2.11.3\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Collecting numpy>=1.20.0\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Collecting typing-extensions>=4.5.0\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.39-cp310-cp310-macosx_14_0_arm64.whl size=2208901 sha256=141ab8d22bd306ec4afdfde6efcb94ca6da420e9de56c266327eeae7f39ccb40\n",
      "  Stored in directory: /Users/kerekovskik/Library/Caches/pip/wheels/23/ee/3c/4a12c19a605d827cf3f67aa4a7d053784959232a964580f626\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "Successfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.39 numpy-1.26.4 typing-extensions-4.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" python3 -m pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#10a020\n"
     ]
    }
   ],
   "source": [
    "# Test GGUF model\n",
    "model_path = \"./color-genius.f16.gguf\"\n",
    "model_path = \"./color-genius.Q8_0.gguf\" # Comment to test FP16 GGUF instead\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "      model_path=\"./color-genius.f16.gguf\",\n",
    "       n_gpu_layers=-1, \n",
    "       n_ctx=2048, \n",
    "       verbose=False\n",
    ")\n",
    "\n",
    "prompt_template = \"<|im_start|>user\\n{msg} <|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "prompt = prompt_template.format(msg=\"Green\")\n",
    "output = llm(\n",
    "      prompt, # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"<|im_end|>\"], # Stop generating just before the model would generate a new question\n",
    "\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHex(color_desc,model) -> str:\n",
    "    prompt_template = \"<|im_start|>user\\n{msg} <|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    prompt = prompt_template.format(msg=color_desc)\n",
    "    llm = Llama(\n",
    "          model_path=model,\n",
    "           n_gpu_layers=-1, \n",
    "           n_ctx=2048, \n",
    "           verbose=False\n",
    "    )\n",
    "\n",
    "    output = llm(\n",
    "          prompt, # Prompt\n",
    "          max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "          stop=[\"<|im_end|>\"], # Stop generating just before the model would generate a new question\n",
    "\n",
    "    ) # Generate a completion, can also call create_completion\n",
    "    color_hex = output[\"choices\"][0][\"text\"]\n",
    "    return color_hex\n",
    "\n",
    "def print_color_space(hex_color):\n",
    "    def hex_to_rgb(hex_color):\n",
    "        hex_color = hex_color.lstrip('#')\n",
    "        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "    r, g, b = hex_to_rgb(hex_color)\n",
    "    print(f'{hex_color}: \\033[48;2;{r};{g};{b}m           \\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ee60b0: \u001b[48;2;238;96;176m           \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./color-genius.f16.gguf\"\n",
    "model_path = \"./color-genius.Q8_0.gguf\" # Comment to test FP16 GGUF instead\n",
    "\n",
    "color_desc = \"Barbie Pink\"\n",
    "\n",
    "print_color_space(getHex(color_desc,model_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_gguf_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
