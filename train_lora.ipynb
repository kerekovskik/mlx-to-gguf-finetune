{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone mlx-examples repo\n",
    "\n",
    "!git clone https://github.com/ml-explore/mlx-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlx>=0.0.7\n",
      "  Using cached mlx-0.1.0-cp310-cp310-macosx_14_0_arm64.whl (17.1 MB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl (393 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl (169 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Using cached tokenizers-0.15.1-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./train_lora_venv/lib/python3.10/site-packages (from transformers->-r ./mlx-examples/lora/requirements.txt (line 2)) (23.2)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.12.25-cp310-cp310-macosx_11_0_arm64.whl (291 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, numpy, mlx, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.20.3 idna-3.6 mlx-0.1.0 numpy-1.26.4 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 tokenizers-0.15.1 tqdm-4.66.1 transformers-4.37.2 typing-extensions-4.9.0 urllib3-2.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -r ./mlx-examples/lora/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Total parameters 1100.868M\n",
      "Trainable parameters 0.819M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.510, Val took 3.359s\n",
      "Iter 10: Train loss 1.827, It/sec 4.432, Tokens/sec 1416.967\n",
      "Iter 20: Train loss 1.254, It/sec 3.983, Tokens/sec 1338.363\n",
      "Iter 30: Train loss 1.095, It/sec 3.880, Tokens/sec 1296.415\n",
      "Iter 40: Train loss 1.155, It/sec 3.997, Tokens/sec 1343.359\n",
      "Iter 50: Train loss 1.021, It/sec 3.888, Tokens/sec 1358.818\n",
      "Iter 60: Train loss 0.967, It/sec 4.092, Tokens/sec 1355.800\n",
      "Iter 70: Train loss 0.979, It/sec 4.060, Tokens/sec 1366.306\n",
      "Iter 80: Train loss 1.009, It/sec 4.016, Tokens/sec 1343.997\n",
      "Iter 90: Train loss 1.049, It/sec 4.019, Tokens/sec 1368.916\n",
      "Iter 100: Train loss 0.988, It/sec 4.101, Tokens/sec 1373.889\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 1.013, It/sec 3.935, Tokens/sec 1324.361\n",
      "Iter 120: Train loss 0.987, It/sec 3.934, Tokens/sec 1311.468\n",
      "Iter 130: Train loss 0.951, It/sec 3.917, Tokens/sec 1338.014\n",
      "Iter 140: Train loss 0.950, It/sec 4.063, Tokens/sec 1350.227\n",
      "Iter 150: Train loss 1.001, It/sec 3.925, Tokens/sec 1323.930\n",
      "Iter 160: Train loss 0.966, It/sec 4.027, Tokens/sec 1362.772\n",
      "Iter 170: Train loss 0.900, It/sec 4.136, Tokens/sec 1312.422\n",
      "Iter 180: Train loss 0.936, It/sec 3.886, Tokens/sec 1315.632\n",
      "Iter 190: Train loss 0.930, It/sec 3.934, Tokens/sec 1311.668\n",
      "Iter 200: Train loss 0.938, It/sec 3.782, Tokens/sec 1263.636\n",
      "Iter 200: Val loss 0.972, Val took 3.623s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 0.920, It/sec 3.874, Tokens/sec 1281.682\n",
      "Iter 220: Train loss 0.931, It/sec 3.944, Tokens/sec 1301.377\n",
      "Iter 230: Train loss 0.974, It/sec 3.956, Tokens/sec 1371.050\n",
      "Iter 240: Train loss 0.943, It/sec 3.776, Tokens/sec 1274.609\n",
      "Iter 250: Train loss 0.913, It/sec 3.868, Tokens/sec 1321.844\n",
      "Iter 260: Train loss 0.908, It/sec 4.071, Tokens/sec 1338.535\n",
      "Iter 270: Train loss 0.916, It/sec 3.900, Tokens/sec 1326.909\n",
      "Iter 280: Train loss 0.932, It/sec 3.958, Tokens/sec 1323.630\n",
      "Iter 290: Train loss 0.889, It/sec 3.985, Tokens/sec 1306.293\n",
      "Iter 300: Train loss 0.945, It/sec 3.906, Tokens/sec 1287.865\n",
      "Iter 300: Saved adapter weights to adapters.npz.\n",
      "Iter 310: Train loss 0.960, It/sec 3.842, Tokens/sec 1331.356\n",
      "Iter 320: Train loss 0.988, It/sec 3.862, Tokens/sec 1306.980\n",
      "Iter 330: Train loss 0.951, It/sec 3.860, Tokens/sec 1283.447\n",
      "Iter 340: Train loss 0.925, It/sec 3.926, Tokens/sec 1302.662\n",
      "Iter 350: Train loss 0.926, It/sec 3.803, Tokens/sec 1283.574\n",
      "Iter 360: Train loss 0.863, It/sec 3.927, Tokens/sec 1289.240\n",
      "Iter 370: Train loss 0.864, It/sec 3.797, Tokens/sec 1255.288\n",
      "Iter 380: Train loss 0.937, It/sec 3.753, Tokens/sec 1292.837\n",
      "Iter 390: Train loss 0.904, It/sec 3.833, Tokens/sec 1296.101\n",
      "Iter 400: Train loss 0.922, It/sec 3.847, Tokens/sec 1306.555\n",
      "Iter 400: Val loss 0.927, Val took 3.614s\n",
      "Iter 400: Saved adapter weights to adapters.npz.\n",
      "Iter 410: Train loss 0.913, It/sec 3.976, Tokens/sec 1302.495\n",
      "Iter 420: Train loss 0.946, It/sec 3.860, Tokens/sec 1294.757\n",
      "Iter 430: Train loss 0.884, It/sec 4.060, Tokens/sec 1359.768\n",
      "Iter 440: Train loss 0.952, It/sec 3.837, Tokens/sec 1331.815\n",
      "Iter 450: Train loss 0.880, It/sec 3.907, Tokens/sec 1297.413\n",
      "Iter 460: Train loss 0.914, It/sec 3.807, Tokens/sec 1272.281\n",
      "Iter 470: Train loss 0.911, It/sec 3.977, Tokens/sec 1315.942\n",
      "Iter 480: Train loss 0.935, It/sec 3.935, Tokens/sec 1299.044\n",
      "Iter 490: Train loss 0.925, It/sec 3.877, Tokens/sec 1315.719\n",
      "Iter 500: Train loss 0.887, It/sec 3.986, Tokens/sec 1322.564\n",
      "Iter 500: Saved adapter weights to adapters.npz.\n",
      "Iter 510: Train loss 0.925, It/sec 3.942, Tokens/sec 1365.276\n",
      "Iter 520: Train loss 0.930, It/sec 3.923, Tokens/sec 1354.906\n",
      "Iter 530: Train loss 0.877, It/sec 3.933, Tokens/sec 1323.961\n",
      "Iter 540: Train loss 0.910, It/sec 3.965, Tokens/sec 1338.954\n",
      "Iter 550: Train loss 0.945, It/sec 3.776, Tokens/sec 1276.759\n",
      "Iter 560: Train loss 0.944, It/sec 3.881, Tokens/sec 1316.376\n",
      "Iter 570: Train loss 0.922, It/sec 4.059, Tokens/sec 1354.359\n",
      "Iter 580: Train loss 0.837, It/sec 3.955, Tokens/sec 1287.328\n",
      "Iter 590: Train loss 0.936, It/sec 3.806, Tokens/sec 1294.530\n",
      "Iter 600: Train loss 0.917, It/sec 3.782, Tokens/sec 1263.348\n",
      "Iter 600: Val loss 0.916, Val took 3.591s\n",
      "Iter 600: Saved adapter weights to adapters.npz.\n",
      "Iter 610: Train loss 0.846, It/sec 3.996, Tokens/sec 1315.151\n",
      "Iter 620: Train loss 0.915, It/sec 3.770, Tokens/sec 1265.752\n",
      "Iter 630: Train loss 0.869, It/sec 3.897, Tokens/sec 1292.287\n",
      "Iter 640: Train loss 0.870, It/sec 3.733, Tokens/sec 1245.655\n",
      "Iter 650: Train loss 0.912, It/sec 3.737, Tokens/sec 1281.864\n",
      "Iter 660: Train loss 0.824, It/sec 3.951, Tokens/sec 1256.106\n",
      "Iter 670: Train loss 0.857, It/sec 3.999, Tokens/sec 1319.194\n",
      "Iter 680: Train loss 0.888, It/sec 3.807, Tokens/sec 1260.875\n",
      "Iter 690: Train loss 0.902, It/sec 3.829, Tokens/sec 1285.090\n",
      "Iter 700: Train loss 0.935, It/sec 3.962, Tokens/sec 1344.322\n",
      "Iter 700: Saved adapter weights to adapters.npz.\n",
      "Iter 710: Train loss 0.922, It/sec 3.752, Tokens/sec 1277.647\n",
      "Iter 720: Train loss 0.908, It/sec 3.786, Tokens/sec 1273.231\n",
      "Iter 730: Train loss 0.902, It/sec 4.010, Tokens/sec 1340.020\n",
      "Iter 740: Train loss 0.904, It/sec 3.874, Tokens/sec 1321.258\n",
      "Iter 750: Train loss 0.908, It/sec 3.932, Tokens/sec 1286.159\n",
      "Iter 760: Train loss 0.846, It/sec 3.711, Tokens/sec 1235.525\n",
      "Iter 770: Train loss 0.926, It/sec 3.777, Tokens/sec 1324.648\n",
      "Iter 780: Train loss 0.879, It/sec 3.810, Tokens/sec 1295.485\n",
      "Iter 790: Train loss 0.855, It/sec 3.876, Tokens/sec 1310.695\n",
      "Iter 800: Train loss 0.864, It/sec 3.756, Tokens/sec 1270.874\n",
      "Iter 800: Val loss 0.903, Val took 3.564s\n",
      "Iter 800: Saved adapter weights to adapters.npz.\n",
      "Iter 810: Train loss 0.932, It/sec 3.958, Tokens/sec 1304.616\n",
      "Iter 820: Train loss 0.890, It/sec 3.805, Tokens/sec 1273.566\n",
      "Iter 830: Train loss 0.868, It/sec 3.748, Tokens/sec 1279.626\n",
      "Iter 840: Train loss 0.933, It/sec 3.970, Tokens/sec 1317.633\n",
      "Iter 850: Train loss 0.903, It/sec 3.984, Tokens/sec 1312.766\n",
      "Iter 860: Train loss 0.934, It/sec 3.787, Tokens/sec 1266.336\n",
      "Iter 870: Train loss 0.906, It/sec 3.783, Tokens/sec 1306.712\n",
      "Iter 880: Train loss 0.940, It/sec 3.886, Tokens/sec 1322.371\n",
      "Iter 890: Train loss 0.835, It/sec 3.981, Tokens/sec 1336.878\n",
      "Iter 900: Train loss 0.880, It/sec 4.026, Tokens/sec 1314.861\n",
      "Iter 900: Saved adapter weights to adapters.npz.\n",
      "Iter 910: Train loss 0.872, It/sec 3.817, Tokens/sec 1297.061\n",
      "Iter 920: Train loss 0.925, It/sec 3.896, Tokens/sec 1314.827\n",
      "Iter 930: Train loss 0.905, It/sec 3.808, Tokens/sec 1307.782\n",
      "Iter 940: Train loss 0.948, It/sec 3.729, Tokens/sec 1279.073\n",
      "Iter 950: Train loss 0.889, It/sec 3.825, Tokens/sec 1291.414\n",
      "Iter 960: Train loss 0.900, It/sec 3.825, Tokens/sec 1298.363\n",
      "Iter 970: Train loss 0.892, It/sec 3.763, Tokens/sec 1312.243\n",
      "Iter 980: Train loss 0.913, It/sec 3.790, Tokens/sec 1312.615\n",
      "Iter 990: Train loss 0.840, It/sec 3.812, Tokens/sec 1255.588\n",
      "Iter 1000: Train loss 0.883, It/sec 3.716, Tokens/sec 1274.585\n",
      "Iter 1000: Val loss 0.895, Val took 3.618s\n",
      "Iter 1000: Saved adapter weights to adapters.npz.\n",
      "Iter 1010: Train loss 0.849, It/sec 3.854, Tokens/sec 1258.430\n",
      "Iter 1020: Train loss 0.827, It/sec 3.844, Tokens/sec 1271.128\n",
      "Iter 1030: Train loss 0.890, It/sec 3.879, Tokens/sec 1306.498\n",
      "Iter 1040: Train loss 0.895, It/sec 3.992, Tokens/sec 1362.941\n",
      "Iter 1050: Train loss 0.897, It/sec 2.918, Tokens/sec 984.428\n",
      "Iter 1060: Train loss 0.900, It/sec 3.943, Tokens/sec 1315.730\n",
      "Iter 1070: Train loss 0.892, It/sec 3.485, Tokens/sec 1193.358\n",
      "Iter 1080: Train loss 0.869, It/sec 3.678, Tokens/sec 1189.029\n",
      "Iter 1090: Train loss 0.935, It/sec 3.842, Tokens/sec 1295.855\n",
      "Iter 1100: Train loss 0.773, It/sec 4.034, Tokens/sec 1304.085\n",
      "Iter 1100: Saved adapter weights to adapters.npz.\n",
      "Iter 1110: Train loss 0.916, It/sec 4.018, Tokens/sec 1369.291\n",
      "Iter 1120: Train loss 0.872, It/sec 3.894, Tokens/sec 1309.102\n",
      "Iter 1130: Train loss 0.837, It/sec 3.893, Tokens/sec 1303.781\n",
      "Iter 1140: Train loss 0.908, It/sec 3.886, Tokens/sec 1359.603\n",
      "Iter 1150: Train loss 0.860, It/sec 3.863, Tokens/sec 1280.436\n",
      "Iter 1160: Train loss 0.839, It/sec 3.963, Tokens/sec 1309.811\n",
      "Iter 1170: Train loss 0.898, It/sec 3.978, Tokens/sec 1354.075\n",
      "Iter 1180: Train loss 0.852, It/sec 4.065, Tokens/sec 1354.444\n",
      "Iter 1190: Train loss 0.847, It/sec 3.894, Tokens/sec 1295.822\n",
      "Iter 1200: Train loss 0.891, It/sec 3.954, Tokens/sec 1331.736\n",
      "Iter 1200: Val loss 0.890, Val took 3.444s\n",
      "Iter 1200: Saved adapter weights to adapters.npz.\n",
      "Iter 1210: Train loss 0.888, It/sec 4.088, Tokens/sec 1416.436\n",
      "Iter 1220: Train loss 0.888, It/sec 4.121, Tokens/sec 1399.596\n",
      "Iter 1230: Train loss 0.861, It/sec 3.978, Tokens/sec 1328.374\n",
      "Iter 1240: Train loss 0.892, It/sec 3.987, Tokens/sec 1329.524\n",
      "Iter 1250: Train loss 0.925, It/sec 3.873, Tokens/sec 1319.798\n",
      "Iter 1260: Train loss 0.932, It/sec 3.814, Tokens/sec 1290.588\n",
      "Iter 1270: Train loss 0.906, It/sec 3.750, Tokens/sec 1270.398\n",
      "Iter 1280: Train loss 0.889, It/sec 3.671, Tokens/sec 1218.821\n",
      "Iter 1290: Train loss 0.816, It/sec 3.913, Tokens/sec 1303.314\n",
      "Iter 1300: Train loss 0.862, It/sec 3.861, Tokens/sec 1295.528\n",
      "Iter 1300: Saved adapter weights to adapters.npz.\n",
      "Iter 1310: Train loss 0.839, It/sec 3.800, Tokens/sec 1269.054\n",
      "Iter 1320: Train loss 0.915, It/sec 3.774, Tokens/sec 1272.530\n",
      "Iter 1330: Train loss 0.898, It/sec 3.790, Tokens/sec 1306.878\n",
      "Iter 1340: Train loss 0.879, It/sec 3.907, Tokens/sec 1297.268\n",
      "Iter 1350: Train loss 0.906, It/sec 3.884, Tokens/sec 1303.027\n",
      "Iter 1360: Train loss 0.883, It/sec 4.148, Tokens/sec 1380.327\n",
      "Iter 1370: Train loss 0.844, It/sec 3.889, Tokens/sec 1283.237\n",
      "Iter 1380: Train loss 0.875, It/sec 3.899, Tokens/sec 1312.445\n",
      "Iter 1390: Train loss 0.855, It/sec 3.794, Tokens/sec 1281.019\n",
      "Iter 1400: Train loss 0.898, It/sec 3.924, Tokens/sec 1329.733\n",
      "Iter 1400: Val loss 0.880, Val took 3.653s\n",
      "Iter 1400: Saved adapter weights to adapters.npz.\n",
      "Iter 1410: Train loss 0.869, It/sec 3.950, Tokens/sec 1357.136\n",
      "Iter 1420: Train loss 0.884, It/sec 4.062, Tokens/sec 1367.161\n",
      "Iter 1430: Train loss 0.841, It/sec 3.696, Tokens/sec 1254.849\n",
      "Iter 1440: Train loss 0.857, It/sec 3.852, Tokens/sec 1306.669\n",
      "Iter 1450: Train loss 0.842, It/sec 4.004, Tokens/sec 1322.012\n",
      "Iter 1460: Train loss 0.871, It/sec 3.933, Tokens/sec 1314.282\n",
      "Iter 1470: Train loss 0.880, It/sec 3.785, Tokens/sec 1275.576\n",
      "Iter 1480: Train loss 0.904, It/sec 3.815, Tokens/sec 1288.456\n",
      "Iter 1490: Train loss 0.817, It/sec 3.974, Tokens/sec 1287.067\n",
      "Iter 1500: Train loss 0.883, It/sec 3.819, Tokens/sec 1296.460\n",
      "Iter 1500: Saved adapter weights to adapters.npz.\n",
      "Iter 1510: Train loss 0.848, It/sec 3.452, Tokens/sec 1174.101\n",
      "Iter 1520: Train loss 0.853, It/sec 3.931, Tokens/sec 1296.139\n",
      "Iter 1530: Train loss 0.899, It/sec 3.945, Tokens/sec 1328.121\n",
      "Iter 1540: Train loss 0.843, It/sec 4.057, Tokens/sec 1331.655\n",
      "Iter 1550: Train loss 0.818, It/sec 4.100, Tokens/sec 1321.070\n",
      "Iter 1560: Train loss 0.873, It/sec 3.894, Tokens/sec 1305.658\n",
      "Iter 1570: Train loss 0.894, It/sec 3.839, Tokens/sec 1336.959\n",
      "Iter 1580: Train loss 0.840, It/sec 3.968, Tokens/sec 1311.121\n",
      "Iter 1590: Train loss 0.854, It/sec 4.077, Tokens/sec 1347.195\n",
      "Iter 1600: Train loss 0.861, It/sec 3.971, Tokens/sec 1338.070\n",
      "Iter 1600: Val loss 0.885, Val took 3.617s\n",
      "Iter 1600: Saved adapter weights to adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "!python3 ./mlx-examples/lora/lora.py --model ./TinyLlama-1.1B-Chat-v1.0-MLX \\\n",
    "    --iters 1600 \\\n",
    "    --max-tokens 2048 \\\n",
    "    --lora-layers 16 \\\n",
    "    --learning-rate 2e-4 \\\n",
    "    --data  ./lora_dataset/ \\\n",
    "    --train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "!python3 ./mlx-examples/lora/fuse.py --model ./TinyLlama-1.1B-Chat-v1.0-MLX \\\n",
    "    --save-path ./TinyLlama-1.1B-Chat-v1.0-color-genius-MLX \\\n",
    "    --de-quantize \\\n",
    "    --adapter-file ./adapters.npz\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_lora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
