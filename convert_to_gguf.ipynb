{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 18872, done.\u001b[K\n",
      "remote: Counting objects: 100% (6381/6381), done.\u001b[K\n",
      "remote: Compressing objects: 100% (466/466), done.\u001b[K\n",
      "remote: Total 18872 (delta 6184), reused 5917 (delta 5915), pack-reused 12491\u001b[K\n",
      "Receiving objects: 100% (18872/18872), 21.25 MiB | 2.51 MiB/s, done.\n",
      "Resolving deltas: 100% (13274/13274), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone llama.cpp repo\n",
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy~=1.24.4 (from -r ./llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
      "  Downloading numpy-1.24.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting sentencepiece~=0.1.98 (from -r ./llama.cpp/./requirements/requirements-convert.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting transformers<5.0.0,>=4.35.2 (from -r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gguf>=0.1.0 (from -r ./llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
      "  Downloading gguf-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.1.1 (from -r ./llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Using cached torch-2.1.2-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Using cached regex-2023.12.25-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions (from torch~=2.1.1->-r ./llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch~=2.1.1->-r ./llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch~=2.1.1->-r ./llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch~=2.1.1->-r ./llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch~=2.1.1->-r ./llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch~=2.1.1->-r ./llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers<5.0.0,>=4.35.2->-r ./llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch~=2.1.1->-r ./llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading numpy-1.24.4-cp310-cp310-macosx_11_0_arm64.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sentencepiece-0.1.99-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m926.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.1.2-cp310-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m930.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl (169 kB)\n",
      "Using cached regex-2023.12.25-cp310-cp310-macosx_11_0_arm64.whl (291 kB)\n",
      "Downloading safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl (393 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, mpmath, urllib3, typing-extensions, tqdm, sympy, safetensors, regex, pyyaml, protobuf, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, gguf, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2024.2.0 gguf-0.6.0 huggingface-hub-0.20.3 idna-3.6 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 numpy-1.24.4 protobuf-4.25.3 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 sentencepiece-0.1.99 sympy-1.12 tokenizers-0.15.2 torch-2.1.2 tqdm-4.66.2 transformers-4.37.2 typing-extensions-4.9.0 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Install llama.cpp requirements \n",
    "\n",
    "!python3 -m pip install -r ./llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file TinyLlama-1.1B-Chat-v1.0-color-genius-MLX/model.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('TinyLlama-1.1B-Chat-v1.0-color-genius-MLX'))\n",
      "Found vocab files: {'tokenizer.model': None, 'vocab.json': None, 'tokenizer.json': PosixPath('TinyLlama-1.1B-Chat-v1.0-color-genius-MLX/tokenizer.json')}\n",
      "Loading vocab file 'TinyLlama-1.1B-Chat-v1.0-color-genius-MLX', type 'hfft'\n",
      "fname_tokenizer: TinyLlama-1.1B-Chat-v1.0-color-genius-MLX\n",
      "Vocab info: <HfVocab with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 2}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32000, 2048]\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 2048]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [2048, 5632]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [5632, 2048]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [5632, 2048]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [256, 2048]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [256, 2048]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [2048, 5632]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [5632, 2048]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [5632, 2048]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [256, 2048]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [256, 2048]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [2048]\n",
      "Writing color-genius.f16.gguf, format 1\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 2\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "gguf: Setting chat_template to {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "[  1/201] Writing tensor output.weight                          | size  32000 x   2048  | type F16  | T+   0\n",
      "[  2/201] Writing tensor token_embd.weight                      | size  32000 x   2048  | type F16  | T+   0\n",
      "[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   0\n",
      "[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   0\n",
      "[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   0\n",
      "[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+   0\n",
      "[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   0\n",
      "[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   0\n",
      "[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   0\n",
      "[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+   0\n",
      "[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+   0\n",
      "Wrote color-genius.f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Convert MLX weights to FP16 GGUF file \n",
    "!python3 llama.cpp/convert.py --vocab-type hfft --concurrency 8 --outfile ./color-genius.f16.gguf ./TinyLlama-1.1B-Chat-v1.0-color-genius-MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is AppleClang 15.0.0.15000100\n",
      "-- The CXX compiler identification is AppleClang 15.0.0.15000100\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.39.3 (Apple Git-145)\") \n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Accelerate framework found\n",
      "-- Metal framework found\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: arm64\n",
      "-- ARM detected\n",
      "-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E\n",
      "-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\n",
      "-- Configuring done (13.5s)\n",
      "-- Generating done (0.3s)\n",
      "-- Build files have been written to: /Users/kerekovskik/repos/mlx-to-gguf-finetune/llama.cpp/build\n",
      "[  1%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-metal.m.o\u001b[0m\n",
      "[  4%] Built target ggml\n",
      "[  5%] \u001b[32m\u001b[1mLinking C static library libggml_static.a\u001b[0m\n",
      "[  5%] Built target ggml_static\n",
      "[  5%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
      "[  6%] Built target llama\n",
      "[  7%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.39.3 (Apple Git-145)\") \n",
      "[  8%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  8%] Built target build_info\n",
      "[  9%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 13%] Built target common\n",
      "[ 13%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 15%] Built target test-quantize-fns\n",
      "[ 16%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 17%] Built target test-quantize-perf\n",
      "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 19%] Built target test-sampling\n",
      "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "[ 21%] Built target test-chat-template\n",
      "[ 21%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-llama\u001b[0m\n",
      "[ 23%] Built target test-tokenizer-0-llama\n",
      "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-falcon\u001b[0m\n",
      "[ 26%] Built target test-tokenizer-0-falcon\n",
      "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-llama\u001b[0m\n",
      "[ 29%] Built target test-tokenizer-1-llama\n",
      "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 31%] Built target test-tokenizer-1-bpe\n",
      "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 33%] Built target test-grammar-parser\n",
      "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 35%] Built target test-llama-grammar\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n",
      "[ 38%] Built target test-grad0\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "[ 40%] Built target test-backend-ops\n",
      "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 43%] Built target test-rope\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "[ 46%] Built target test-model-load-cancel\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "[ 49%] Built target test-autorelease\n",
      "[ 50%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-c\u001b[0m\n",
      "[ 51%] Built target test-c\n",
      "[ 51%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
      "[ 52%] Built target baby-llama\n",
      "[ 53%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched\u001b[0m\n",
      "[ 54%] Built target batched\n",
      "[ 54%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched-bench\u001b[0m\n",
      "[ 55%] Built target batched-bench\n",
      "[ 56%] \u001b[32mBuilding CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/beam-search\u001b[0m\n",
      "[ 57%] Built target beam-search\n",
      "[ 58%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
      "[ 58%] Built target benchmark\n",
      "[ 59%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/convert-llama2c-to-ggml\u001b[0m\n",
      "[ 59%] Built target convert-llama2c-to-ggml\n",
      "[ 60%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
      "[ 61%] Built target embedding\n",
      "[ 61%] \u001b[32mBuilding CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/finetune\u001b[0m\n",
      "[ 62%] Built target finetune\n",
      "[ 63%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/infill\u001b[0m\n",
      "[ 64%] Built target infill\n",
      "[ 65%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 66%] Built target llama-bench\n",
      "[ 67%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
      "[ 67%] Built target llava\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
      "[ 68%] Built target llava_static\n",
      "[ 69%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llava-cli\u001b[0m\n",
      "[ 70%] Built target llava-cli\n",
      "[ 71%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
      "[ 72%] Built target main\n",
      "[ 73%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/tokenize\u001b[0m\n",
      "[ 73%] Built target tokenize\n",
      "[ 73%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/parallel\u001b[0m\n",
      "[ 74%] Built target parallel\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
      "[ 75%] Built target perplexity\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
      "[ 77%] Built target quantize\n",
      "[ 77%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
      "[ 78%] Built target quantize-stats\n",
      "[ 79%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
      "[ 80%] Built target save-load-state\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/simple\u001b[0m\n",
      "[ 82%] Built target simple\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/passkey\u001b[0m\n",
      "[ 84%] Built target passkey\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/speculative\u001b[0m\n",
      "[ 85%] Built target speculative\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lookahead\u001b[0m\n",
      "[ 86%] Built target lookahead\n",
      "[ 87%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lookup\u001b[0m\n",
      "[ 88%] Built target lookup\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gguf\u001b[0m\n",
      "[ 89%] Built target gguf\n",
      "[ 90%] \u001b[32mBuilding CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/train-text-from-scratch\u001b[0m\n",
      "[ 91%] Built target train-text-from-scratch\n",
      "[ 92%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/imatrix\u001b[0m\n",
      "[ 93%] Built target imatrix\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/server.dir/server.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/server\u001b[0m\n",
      "[ 94%] Built target server\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/export-lora\u001b[0m\n",
      "[ 96%] Built target export-lora\n",
      "[ 97%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
      "[ 98%] Built target vdot\n",
      "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
      "[100%] Built target q8dot\n"
     ]
    }
   ],
   "source": [
    "# Compile llama.cpp Program\n",
    "! cd llama.cpp ; mkdir build; cd build; cmake .. ; cmake --build . --config Release\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 2212 (633782b8)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0\n",
      "main: quantizing './color-genius.f16.gguf' to './color-genius.Q8_0.gguf' as Q8_0 using 8 threads\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from ./color-genius.f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type  f16:  156 tensors\n",
      "llama_model_quantize_internal: meta size = 736288 bytes\n",
      "[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, quantizing to q8_0 .. size =   125.00 MiB ->    66.41 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.109 0.239 0.108 0.087 0.064 0.045 0.029 0.018 0.025 \n",
      "[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, quantizing to q8_0 .. size =   125.00 MiB ->    66.41 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[   3/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   4/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[   5/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.232 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
      "[   6/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[   7/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   8/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.020 0.007 0.011 0.017 0.027 0.047 0.096 0.553 0.095 0.047 0.027 0.017 0.010 0.007 0.020 \n",
      "[   9/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.062 0.086 0.111 0.256 0.111 0.086 0.062 0.043 0.027 0.017 0.026 \n",
      "[  10/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.021 0.009 0.014 0.022 0.036 0.059 0.105 0.467 0.105 0.059 0.036 0.022 0.014 0.009 0.021 \n",
      "[  11/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.017 0.027 0.041 0.058 0.080 0.106 0.288 0.106 0.080 0.058 0.041 0.027 0.018 0.026 \n",
      "[  12/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  13/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  14/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  15/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  16/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  17/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.024 0.014 0.023 0.036 0.054 0.079 0.108 0.322 0.109 0.079 0.054 0.036 0.023 0.014 0.024 \n",
      "[  18/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  19/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.025 0.015 0.024 0.038 0.055 0.079 0.106 0.315 0.106 0.079 0.055 0.038 0.024 0.015 0.025 \n",
      "[  20/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.017 0.027 0.041 0.058 0.083 0.110 0.277 0.110 0.082 0.059 0.041 0.027 0.017 0.026 \n",
      "[  21/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  22/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  23/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[  24/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  25/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  26/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.019 0.031 0.046 0.066 0.088 0.107 0.233 0.108 0.089 0.065 0.047 0.030 0.019 0.027 \n",
      "[  27/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  28/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
      "[  29/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.066 0.089 0.108 0.235 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[  30/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  31/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.106 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  32/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  33/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  34/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  35/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.087 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[  36/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[  37/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.235 0.109 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[  38/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.019 0.030 0.045 0.064 0.088 0.109 0.239 0.109 0.087 0.065 0.045 0.030 0.019 0.027 \n",
      "[  39/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  40/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  41/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  42/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  43/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  44/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.047 0.066 0.087 0.108 0.232 0.108 0.087 0.065 0.046 0.031 0.019 0.027 \n",
      "[  45/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  46/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  47/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.109 0.238 0.108 0.086 0.064 0.045 0.030 0.019 0.027 \n",
      "[  48/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  49/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.030 0.047 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.047 0.030 0.019 0.027 \n",
      "[  50/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  51/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  52/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  53/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.087 0.067 0.047 0.031 0.019 0.027 \n",
      "[  54/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[  55/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  56/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.087 0.108 0.236 0.107 0.088 0.065 0.045 0.030 0.019 0.027 \n",
      "[  57/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  58/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[  59/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.108 0.231 0.107 0.088 0.065 0.047 0.030 0.019 0.027 \n",
      "[  60/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.030 0.047 0.065 0.088 0.108 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  61/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  62/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.026 \n",
      "[  63/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  64/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.108 0.232 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[  65/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.018 0.030 0.045 0.065 0.086 0.109 0.239 0.109 0.088 0.065 0.045 0.030 0.019 0.027 \n",
      "[  66/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  67/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  68/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
      "[  69/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  70/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  71/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.019 0.030 0.046 0.065 0.088 0.107 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[  72/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[  73/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[  74/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.017 0.027 0.043 0.062 0.087 0.111 0.253 0.111 0.085 0.063 0.043 0.027 0.017 0.026 \n",
      "[  75/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  76/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  77/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.228 0.106 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  78/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  79/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  80/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  81/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[  82/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.234 0.108 0.087 0.066 0.046 0.030 0.019 0.027 \n",
      "[  83/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.087 0.108 0.236 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[  84/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  85/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[  86/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.107 0.227 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  87/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[  88/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  89/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[  90/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[  91/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[  92/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.065 0.087 0.110 0.241 0.108 0.087 0.064 0.045 0.030 0.018 0.027 \n",
      "[  93/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  94/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
      "[  95/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.087 0.066 0.048 0.031 0.020 0.027 \n",
      "[  96/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[  97/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  98/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.029 0.045 0.065 0.087 0.108 0.238 0.109 0.087 0.065 0.046 0.030 0.018 0.026 \n",
      "[  99/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[ 100/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 101/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.017 0.027 0.042 0.062 0.087 0.112 0.256 0.112 0.087 0.062 0.042 0.027 0.016 0.026 \n",
      "[ 102/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 103/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 104/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[ 105/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[ 106/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 107/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.018 0.029 0.046 0.065 0.088 0.109 0.239 0.109 0.087 0.065 0.046 0.029 0.018 0.027 \n",
      "[ 108/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 109/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 110/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.029 0.044 0.064 0.087 0.109 0.241 0.109 0.087 0.064 0.044 0.029 0.019 0.026 \n",
      "[ 111/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 112/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 113/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 114/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 115/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 116/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.017 0.027 0.042 0.062 0.087 0.113 0.254 0.113 0.087 0.062 0.042 0.027 0.017 0.026 \n",
      "[ 117/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 118/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 119/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.018 0.029 0.044 0.064 0.086 0.107 0.249 0.107 0.086 0.063 0.044 0.030 0.019 0.027 \n",
      "[ 120/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 121/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.106 0.228 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 122/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[ 123/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 124/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 125/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.110 0.241 0.110 0.087 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 126/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 127/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.107 0.232 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 128/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.065 0.086 0.107 0.233 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
      "[ 129/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 130/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.088 0.110 0.242 0.110 0.088 0.064 0.044 0.029 0.018 0.026 \n",
      "[ 131/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.107 0.228 0.106 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 132/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[ 133/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 134/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.019 0.030 0.045 0.065 0.087 0.108 0.238 0.109 0.088 0.064 0.046 0.030 0.019 0.027 \n",
      "[ 135/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 136/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.108 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 137/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.047 0.066 0.087 0.107 0.233 0.108 0.087 0.066 0.046 0.030 0.019 0.026 \n",
      "[ 138/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 139/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 140/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[ 141/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 142/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 143/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.110 0.239 0.110 0.088 0.065 0.045 0.029 0.018 0.026 \n",
      "[ 144/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 145/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.235 0.107 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 146/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.047 0.066 0.087 0.106 0.238 0.106 0.086 0.065 0.047 0.031 0.019 0.027 \n",
      "[ 147/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 148/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 149/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.105 0.088 0.066 0.048 0.032 0.020 0.027 \n",
      "[ 150/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[ 151/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 152/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.088 0.109 0.241 0.110 0.087 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 153/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 154/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.236 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 155/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.064 0.087 0.107 0.238 0.107 0.087 0.065 0.046 0.031 0.019 0.027 \n",
      "[ 156/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 157/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
      "[ 158/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 159/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 160/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 161/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.018 0.030 0.045 0.065 0.088 0.109 0.240 0.109 0.087 0.064 0.045 0.029 0.018 0.027 \n",
      "[ 162/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 163/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.107 0.234 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 164/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.020 0.030 0.046 0.065 0.086 0.106 0.237 0.107 0.086 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 165/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 166/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 167/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 168/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 169/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 170/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.109 0.239 0.109 0.089 0.064 0.046 0.029 0.018 0.027 \n",
      "[ 171/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.107 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 172/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.236 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 173/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.064 0.087 0.108 0.238 0.108 0.087 0.064 0.046 0.030 0.019 0.027 \n",
      "[ 174/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 175/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.233 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 176/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 177/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.087 0.107 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 178/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 179/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.232 0.108 0.088 0.065 0.047 0.030 0.019 0.027 \n",
      "[ 180/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 181/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.232 0.107 0.087 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 182/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.087 0.108 0.241 0.108 0.086 0.064 0.045 0.030 0.019 0.027 \n",
      "[ 183/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 184/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 185/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
      "[ 186/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 187/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 188/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.110 0.242 0.109 0.087 0.064 0.045 0.030 0.018 0.026 \n",
      "[ 189/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 190/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.087 0.109 0.241 0.108 0.086 0.064 0.045 0.029 0.019 0.027 \n",
      "[ 191/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.236 0.108 0.087 0.065 0.046 0.031 0.019 0.027 \n",
      "[ 192/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 193/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 194/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 \n",
      "[ 195/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.00 MiB ->    11.69 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 196/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 197/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.047 0.030 0.019 0.027 \n",
      "[ 198/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
      "[ 199/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.046 0.031 0.019 0.027 \n",
      "[ 200/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.00 MiB ->     0.53 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.086 0.108 0.238 0.108 0.087 0.065 0.045 0.030 0.019 0.027 \n",
      "[ 201/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "llama_model_quantize_internal: model size  =  2098.35 MB\n",
      "llama_model_quantize_internal: quant size  =  1114.91 MB\n",
      "llama_model_quantize_internal: hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.232 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
      "\n",
      "main: quantize time =  1855.19 ms\n",
      "main:    total time =  1855.19 ms\n"
     ]
    }
   ],
   "source": [
    "# Optionally quantize the model\n",
    "!llama.cpp/build/bin/quantize ./color-genius.f16.gguf ./color-genius.Q8_0.gguf Q8_0 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
